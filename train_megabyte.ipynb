{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataclasses import dataclass\n",
    "from model import Patch_Embedder, GlobalModel, LocalModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Megabyte_Config:\n",
    "    debug : bool = False\n",
    "    dtype : torch.dtype = torch.float16\n",
    "\n",
    "    #initialization\n",
    "    init_range: float = 0.02\n",
    "    layer_norm_eps: float = 1e-5\n",
    "\n",
    "    #patch_embedder\n",
    "    patch_size: int = 4\n",
    "\n",
    "    #global model\n",
    "    global_d_pre_patch: int = 32\n",
    "    global_d_model =  global_d_pre_patch * patch_size\n",
    "    global_n_heads = 8\n",
    "    global_d_head = 8\n",
    "    global_n_layers = 2\n",
    "    global_d_mlp = 64\n",
    "\n",
    "    d_vocab : int = 256\n",
    "    \n",
    "    #local model\n",
    "    #global_dropout = 0.1\n",
    "    local_d_model = 16\n",
    "    local_n_heads = 4\n",
    "    local_d_head = 4\n",
    "    local_n_layers = 2\n",
    "    local_d_mlp = 8\n",
    "\n",
    "    #task\n",
    "    classification : bool = True\n",
    "    n_classes = 10\n",
    "\n",
    "#TODO: should there be special bytes for image_start, image_end, text_start, text_end?\n",
    "\n",
    "class Megabyte(nn.Module):\n",
    "    def __init__(self, cfg: Megabyte_Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self._name = \"Megabyte\"\n",
    "        self.patch_embedder = Patch_Embedder(cfg)\n",
    "        self.global_model = GlobalModel(cfg)\n",
    "        self.local_model = LocalModel(cfg)\n",
    "        self.local_pad = nn.Parameter(torch.randn(1, 1, cfg.local_d_model))\n",
    "        self.global_to_local_proj = nn.Linear(cfg.global_d_pre_patch, cfg.local_d_model)\n",
    "        self.byte_embedding_local = nn.Embedding(256, cfg.local_d_model)\n",
    "        self.unembed = nn.Linear(cfg.local_d_model, 256)\n",
    "\n",
    "        if self.cfg.classification:\n",
    "            self.global_class_token = nn.Parameter(torch.randn(1, cfg.global_d_model))\n",
    "            self.local_class_token = nn.Parameter(torch.randn(1, cfg.local_d_model))\n",
    "            self.classification_head = nn.Linear(cfg.local_d_model, cfg.n_classes)\n",
    "\n",
    "    def get_param_count(self) -> int:\n",
    "        '''returns the number of parameters in the model'''\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad) # all params with gradients\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Embedding and global model processing\n",
    "        if self.cfg.classification:\n",
    "            #add class token\n",
    "            batch_size = x.shape[0]\n",
    "            global_class_token = self.global_class_token.unsqueeze(0).repeat(batch_size, 1, 1)#.t  # Shape: [batch_size, 1, local_d_model]\n",
    "            local_class_token = self.local_class_token.unsqueeze(0).repeat(batch_size, self.cfg.patch_size, 1)#.transpose(1,2)  # Shape: [batch_size, 1, local_d_model]\n",
    "\n",
    "        #print(\"input x.shape\", x.shape)\n",
    "        #compute bytes_embedding for local model and offset by 1\n",
    "        byte_embeddings_local = self.byte_embedding_local(x) # dim local_d_model\n",
    "\n",
    "        # Insert class token at the start of the sequence\n",
    "\n",
    "        if self.cfg.classification:\n",
    "            byte_embeddings_local = torch.cat([local_class_token, byte_embeddings_local], dim=1)\n",
    "        #print(\"after cat byte_embeddings_local.shape\", byte_embeddings_local.shape)\n",
    "        if self.cfg.debug : print(\"byte_embeddings_local.shape\", byte_embeddings_local.shape)\n",
    "        offset_byte_embeddings_local = F.pad(byte_embeddings_local, (0, 0, 1, 0), \"constant\", 0)\n",
    "        if self.cfg.debug :  print(\"offset_byte_embeddings_local.shape\", offset_byte_embeddings_local.shape)\n",
    "        offset_byte_embeddings_local[:, 0, :] = self.local_pad\n",
    "        offset_byte_embeddings_local = offset_byte_embeddings_local[:, :-1, :] # remove last byte\n",
    "    \n",
    "        #print(\"offset_byte_embeddings_local.shape\", offset_byte_embeddings_local.shape)\n",
    "    \n",
    "        if self.cfg.debug : print(\"input tensor\", x.shape)\n",
    "        embedded = self.patch_embedder(x) \n",
    "        # add class token\n",
    "        #print(\"pre embedded.shape\", embedded.shape)\n",
    "        if self.cfg.classification:\n",
    "            embedded = torch.cat([global_class_token, embedded], dim=1)\n",
    "        #print(\"post embedded.shape\", embedded.shape)\n",
    "        global_out = self.global_model(embedded)\n",
    "        batch_size, num_patches, _ = global_out.shape\n",
    "\n",
    "\n",
    "        reshaped = global_out.view(batch_size, num_patches, self.cfg.patch_size , self.cfg.global_d_pre_patch)\n",
    "        if self.cfg.debug : print(\"shape offset_byte_embeddings_local\", offset_byte_embeddings_local.shape)\n",
    "        offset_byte_embeddings_local = offset_byte_embeddings_local.view(batch_size, num_patches, self.cfg.patch_size , self.cfg.local_d_model)\n",
    "\n",
    "        if self.cfg.debug : print(\"reshaped.shape\", reshaped.shape)\n",
    "        # Project each position to the dimension of the local model\n",
    "        projected = self.global_to_local_proj(reshaped)\n",
    "\n",
    "        if self.cfg.debug : print(\"projected.shape\", projected.shape)\n",
    "        # Combine with byte embeddings\n",
    "        if self.cfg.debug : print(\"offset_byte_embeddings_local.shape\", offset_byte_embeddings_local.shape)\n",
    "        if self.cfg.debug : print(\"projected.shape\", projected.shape)\n",
    "        \n",
    "        combined = projected + offset_byte_embeddings_local\n",
    "\n",
    "        # Process with local model\n",
    "        if self.cfg.debug :  print(\"combined.shape\", combined.shape)\n",
    "        local_out = self.local_model(combined) # shape [batch, n_patches, patch_size, local_d_model]\n",
    "        unembedded = self.unembed(local_out)\n",
    "\n",
    "        batch_size, num_patches, patch_size, d_local_model = unembedded.shape\n",
    "        unembedded_flat = unembedded.view(batch_size * num_patches * patch_size, d_local_model)\n",
    "        # Apply softmax to compute probability distribution over the vocabulary\n",
    "        probs_flat = F.softmax(unembedded_flat, dim=-1)\n",
    "        if self.cfg.debug :  print(\"probs_flat.shape\", probs_flat.shape)\n",
    "        # Reshape back to original shape\n",
    "        probs = probs_flat.reshape(batch_size, num_patches, patch_size, d_local_model)\n",
    "        if self.cfg.debug : print(\"probs.shape\", probs.shape) \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bytes(texts: list[str]) -> torch.Tensor:\n",
    "    '''converts text to bytes and returns [batch, seq_len] tensor \n",
    "    and pads to max_seq_len in batch with zeros'''\n",
    "    return torch.nn.utils.rnn.pad_sequence([torch.Tensor([ord(c) for c in text]).to(dtype=torch.long) for text in texts], batch_first=True)\n",
    "\n",
    "def bytes_to_text(bytes: torch.Tensor) -> list[str]:\n",
    "    '''converts bytes in torch.Tensor to text'''\n",
    "    texts = []\n",
    "    bytes = bytes.to(dtype=torch.uint8)\n",
    "    for i in range(bytes.size(0)): # iter over batch\n",
    "        texts.append(''.join([chr(b) for b in bytes[i].tolist()]))\n",
    "    return texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def megabyte_collate_fn(batch, type=\"image\"):\n",
    "    if type == \"image\":\n",
    "        images, labels = zip(*batch)\n",
    "        images = np.stack(images) # we need it to be a numpy array to use patch_images\n",
    "        bytes = img_to_bytes(images) # should be integer type\n",
    "        labels = torch.Tensor(labels).to(torch.int64)\n",
    "        #print(\"bytes.shape\", bytes.shape)\n",
    "    elif type == \"text\":\n",
    "        texts, labels = zip(*batch)\n",
    "        bytes = torch.Tensor(text_to_bytes(texts))\n",
    "    return bytes, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "from utils import text_to_bytes, bytes_to_text\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "text = \"from marcy to madison square\"\n",
    "bytes = text_to_bytes([text, \"to aim at yout, you to smithereens, cock sucker take one from your team and i need you to rememebr one thing\"])\n",
    "model = Megabyte(Megabyte_Config())\n",
    "out = model.forward(bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([216, 256])\n",
      "torch.Size([216])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)\n",
    "print(bytes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape torch.Size([216, 256])\n",
      "bytes.shape torch.Size([216])\n",
      "loss tensor(5.5448, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "collate_fn = partial(megabyte_collate_fn, type=\"text\")\n",
    "#load dataset \n",
    "dataset = datasets.load_dataset('tiny_shakespeare')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "epochs = 1\n",
    "device = \"mps\"\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        text_bytes = batch.to(device) # ?\n",
    "        out = model.forward(text_bytes)\n",
    "        batch_dim, patch_dim, local_dim, token= out.shape\n",
    "        out = out[:,1:, : , :].reshape(batch_dim * (patch_dim-1)*local_dim, token) # resha\n",
    "        text_bytes = bytes.view(-1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
